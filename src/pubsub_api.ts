// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.3
//   protoc               v5.28.2
// source: src/pubsub_api.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import {
  type CallOptions,
  ChannelCredentials,
  Client,
  ClientDuplexStream,
  type ClientOptions,
  type ClientUnaryCall,
  handleBidiStreamingCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";

export const protobufPackage = "eventbus.v1";

/** Supported error codes */
export enum ErrorCode {
  UNKNOWN = 0,
  PUBLISH = 1,
  /** COMMIT - ErrorCode for unrecoverable commit errors. */
  COMMIT = 2,
  UNRECOGNIZED = -1,
}

export function errorCodeFromJSON(object: any): ErrorCode {
  switch (object) {
    case 0:
    case "UNKNOWN":
      return ErrorCode.UNKNOWN;
    case 1:
    case "PUBLISH":
      return ErrorCode.PUBLISH;
    case 2:
    case "COMMIT":
      return ErrorCode.COMMIT;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ErrorCode.UNRECOGNIZED;
  }
}

export function errorCodeToJSON(object: ErrorCode): string {
  switch (object) {
    case ErrorCode.UNKNOWN:
      return "UNKNOWN";
    case ErrorCode.PUBLISH:
      return "PUBLISH";
    case ErrorCode.COMMIT:
      return "COMMIT";
    case ErrorCode.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/**
 * Supported subscription replay start values.
 * By default, the subscription will start at the tip of the stream if ReplayPreset is not specified.
 */
export enum ReplayPreset {
  /** LATEST - Start the subscription at the tip of the stream. */
  LATEST = 0,
  /** EARLIEST - Start the subscription at the earliest point in the stream. */
  EARLIEST = 1,
  /** CUSTOM - Start the subscription after a custom point in the stream. This must be set with a valid replay_id in the FetchRequest. */
  CUSTOM = 2,
  UNRECOGNIZED = -1,
}

export function replayPresetFromJSON(object: any): ReplayPreset {
  switch (object) {
    case 0:
    case "LATEST":
      return ReplayPreset.LATEST;
    case 1:
    case "EARLIEST":
      return ReplayPreset.EARLIEST;
    case 2:
    case "CUSTOM":
      return ReplayPreset.CUSTOM;
    case -1:
    case "UNRECOGNIZED":
    default:
      return ReplayPreset.UNRECOGNIZED;
  }
}

export function replayPresetToJSON(object: ReplayPreset): string {
  switch (object) {
    case ReplayPreset.LATEST:
      return "LATEST";
    case ReplayPreset.EARLIEST:
      return "EARLIEST";
    case ReplayPreset.CUSTOM:
      return "CUSTOM";
    case ReplayPreset.UNRECOGNIZED:
    default:
      return "UNRECOGNIZED";
  }
}

/** Contains information about a topic and uniquely identifies it. TopicInfo is returned by the GetTopic RPC method. */
export interface TopicInfo {
  /** Topic name */
  topicName: string;
  /** Tenant/org GUID */
  tenantGuid: string;
  /** Is publishing allowed? */
  canPublish: boolean;
  /** Is subscription allowed? */
  canSubscribe: boolean;
  /**
   * ID of the current topic schema, which can be used for
   * publishing of generically serialized events.
   */
  schemaId: string;
  /** RPC ID used to trace errors. */
  rpcId: string;
}

/**
 * A request message for GetTopic. Note that the tenant/org is not directly referenced
 * in the request, but is implicitly identified by the authentication headers.
 */
export interface TopicRequest {
  /** The name of the topic to retrieve. */
  topicName: string;
}

/**
 * Reserved for future use.
 * Header that contains information for distributed tracing, filtering, routing, etc.
 * For example, X-B3-* headers assigned by a publisher are stored with the event and
 * can provide a full distributed trace of the event across its entire lifecycle.
 */
export interface EventHeader {
  key: string;
  value: Buffer;
}

/** Represents an event that an event publishing app creates. */
export interface ProducerEvent {
  /** Either a user-provided ID or a system generated guid */
  id: string;
  /** Schema fingerprint for this event which is hash of the schema */
  schemaId: string;
  /** The message data field */
  payload: Buffer;
  /** Reserved for future use. Key-value pairs of headers. */
  headers: EventHeader[];
}

/**
 * Represents an event that is consumed in a subscriber client.
 * In addition to the fields in ProducerEvent, ConsumerEvent has the replay_id field.
 */
export interface ConsumerEvent {
  /** The event with fields identical to ProducerEvent */
  event:
    | ProducerEvent
    | undefined;
  /**
   * The replay ID of the event.
   * A subscriber app can store the replay ID. When the app restarts, it can resume subscription
   * starting from events in the event bus after the event with that replay ID.
   */
  replayId: Buffer;
}

/** Event publish result that the Publish RPC method returns. The result contains replay_id or a publish error. */
export interface PublishResult {
  /** Replay ID of the event */
  replayId: Buffer;
  /** Publish error if any */
  error:
    | Error
    | undefined;
  /** Correlation key of the ProducerEvent */
  correlationKey: string;
}

/** Contains error information for an error that an RPC method returns. */
export interface Error {
  /** Error code */
  code: ErrorCode;
  /** Error message */
  msg: string;
}

/**
 * Request for the Subscribe streaming RPC method. This request is used to:
 * 1. Establish the initial subscribe stream.
 * 2. Request more events from the subscription stream.
 * Flow Control is handled by the subscriber via num_requested.
 * A client can specify a starting point for the subscription with replay_preset and replay_id combinations.
 * If no replay_preset is specified, the subscription starts at LATEST (tip of the stream).
 * replay_preset and replay_id values are only consumed as part of the first FetchRequest. If
 * a client needs to start at another point in the stream, it must start a new subscription.
 */
export interface FetchRequest {
  /**
   * Identifies a topic for subscription in the very first FetchRequest of the stream. The topic cannot change
   * in subsequent FetchRequests within the same subscribe stream, but can be omitted for efficiency.
   */
  topicName: string;
  /**
   * Subscription starting point. This is consumed only as part of the first FetchRequest
   * when the subscription is set up.
   */
  replayPreset: ReplayPreset;
  /**
   * If replay_preset of CUSTOM is selected, specify the subscription point to start after.
   * This is consumed only as part of the first FetchRequest when the subscription is set up.
   */
  replayId: Buffer;
  /**
   * Number of events a client is ready to accept. Each subsequent FetchRequest informs the server
   * of additional processing capacity available on the client side. There is no guarantee of equal number of
   * FetchResponse messages to be sent back. There is not necessarily a correspondence between
   * number of requested events in FetchRequest and the number of events returned in subsequent
   * FetchResponses.
   */
  numRequested: number;
  /** For internal Salesforce use only. */
  authRefresh: string;
}

/**
 * Response for the Subscribe streaming RPC method. This returns ConsumerEvent(s).
 * If there are no events to deliver, the server sends an empty batch fetch response with the latest replay ID. The
 * empty fetch response is sent within 270 seconds. An empty fetch response provides a periodic keepalive from the
 * server and the latest replay ID.
 */
export interface FetchResponse {
  /** Received events for subscription for client consumption */
  events: ConsumerEvent[];
  /**
   * Latest replay ID of a subscription. Enables clients with an updated replay value so that they can keep track
   * of their last consumed replay. Clients will not have to start a subscription at a very old replay in the case where a resubscribe is necessary.
   */
  latestReplayId: Buffer;
  /** RPC ID used to trace errors. */
  rpcId: string;
  /** Number of remaining events to be delivered to the client for a Subscribe RPC call. */
  pendingNumRequested: number;
}

/** Request for the GetSchema RPC method. The schema request is based on the event schema ID. */
export interface SchemaRequest {
  /** Schema fingerprint for this event, which is a hash of the schema. */
  schemaId: string;
}

/** Response for the GetSchema RPC method. This returns the schema ID and schema of an event. */
export interface SchemaInfo {
  /** Avro schema in JSON format */
  schemaJson: string;
  /** Schema fingerprint */
  schemaId: string;
  /** RPC ID used to trace errors. */
  rpcId: string;
}

/** Request for the Publish and PublishStream RPC method. */
export interface PublishRequest {
  /** Topic to publish on */
  topicName: string;
  /** Batch of ProducerEvent(s) to send */
  events: ProducerEvent[];
  /** For internal Salesforce use only. */
  authRefresh: string;
}

/**
 * Response for the Publish and PublishStream RPC methods. This returns
 * a list of PublishResults for each event that the client attempted to
 * publish. PublishResult indicates if publish succeeded or not
 * for each event. It also returns the schema ID that was used to create
 * the ProducerEvents in the PublishRequest.
 */
export interface PublishResponse {
  /** Publish results */
  results: PublishResult[];
  /** Schema fingerprint for this event, which is a hash of the schema */
  schemaId: string;
  /** RPC ID used to trace errors. */
  rpcId: string;
}

/**
 * This feature is part of an open beta release and is subject to the applicable
 * Beta Services Terms provided at Agreements and Terms
 * (https://www.salesforce.com/company/legal/agreements/).
 *
 * Request for the ManagedSubscribe streaming RPC method. This request is used to:
 * 1. Establish the initial managed subscribe stream.
 * 2. Request more events from the subscription stream.
 * 3. Commit a Replay ID using CommitReplayRequest.
 */
export interface ManagedFetchRequest {
  /**
   * Managed subscription ID or developer name. This value corresponds to the
   * ID or developer name of the ManagedEventSubscription Tooling API record.
   * This value is consumed as part of the first ManagedFetchRequest only.
   * The subscription_id cannot change in subsequent ManagedFetchRequests
   * within the same subscribe stream, but can be omitted for efficiency.
   */
  subscriptionId: string;
  developerName: string;
  /**
   * Number of events a client is ready to accept. Each subsequent FetchRequest informs the server
   * of additional processing capacity available on the client side. There is no guarantee of equal number of
   * FetchResponse messages to be sent back. There is not necessarily a correspondence between
   * number of requested events in FetchRequest and the number of events returned in subsequent
   * FetchResponses.
   */
  numRequested: number;
  /** For internal Salesforce use only. */
  authRefresh: string;
  commitReplayIdRequest: CommitReplayRequest | undefined;
}

/**
 * This feature is part of an open beta release and is subject to the applicable
 * Beta Services Terms provided at Agreements and Terms
 * (https://www.salesforce.com/company/legal/agreements/).
 *
 * Response for the ManagedSubscribe streaming RPC method. This can return
 * ConsumerEvent(s) or CommitReplayResponse along with other metadata.
 */
export interface ManagedFetchResponse {
  /** Received events for subscription for client consumption */
  events: ConsumerEvent[];
  /** Latest replay ID of a subscription. */
  latestReplayId: Buffer;
  /** RPC ID used to trace errors. */
  rpcId: string;
  /** Number of remaining events to be delivered to the client for a Subscribe RPC call. */
  pendingNumRequested: number;
  /** commit response */
  commitResponse: CommitReplayResponse | undefined;
}

/**
 * This feature is part of an open beta release and is subject to the applicable
 * Beta Services Terms provided at Agreements and Terms
 * (https://www.salesforce.com/company/legal/agreements/).
 *
 * Request to commit a Replay ID for the last processed event or for the latest
 * replay ID received in an empty batch of events.
 */
export interface CommitReplayRequest {
  /** commit_request_id to identify commit responses */
  commitRequestId: string;
  /** replayId to commit */
  replayId: Buffer;
}

/**
 * This feature is part of an open beta release and is subject to the applicable
 * Beta Services Terms provided at Agreements and Terms
 * (https://www.salesforce.com/company/legal/agreements/).
 *
 * There is no guaranteed 1:1 CommitReplayRequest to CommitReplayResponse.
 * N CommitReplayRequest(s) can get compressed in a batch resulting in a single
 * CommitReplayResponse which reflects the latest values of last
 * CommitReplayRequest in that batch.
 */
export interface CommitReplayResponse {
  /** commit_request_id to identify commit responses. */
  commitRequestId: string;
  /** replayId that may have been committed */
  replayId: Buffer;
  /** for failed commits */
  error:
    | Error
    | undefined;
  /** time when server received request in epoch ms */
  processTime: number;
}

function createBaseTopicInfo(): TopicInfo {
  return { topicName: "", tenantGuid: "", canPublish: false, canSubscribe: false, schemaId: "", rpcId: "" };
}

export const TopicInfo: MessageFns<TopicInfo> = {
  encode(message: TopicInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.topicName !== "") {
      writer.uint32(10).string(message.topicName);
    }
    if (message.tenantGuid !== "") {
      writer.uint32(18).string(message.tenantGuid);
    }
    if (message.canPublish !== false) {
      writer.uint32(24).bool(message.canPublish);
    }
    if (message.canSubscribe !== false) {
      writer.uint32(32).bool(message.canSubscribe);
    }
    if (message.schemaId !== "") {
      writer.uint32(42).string(message.schemaId);
    }
    if (message.rpcId !== "") {
      writer.uint32(50).string(message.rpcId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TopicInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTopicInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.topicName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.tenantGuid = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.canPublish = reader.bool();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.canSubscribe = reader.bool();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.schemaId = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.rpcId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TopicInfo {
    return {
      topicName: isSet(object.topicName) ? globalThis.String(object.topicName) : "",
      tenantGuid: isSet(object.tenantGuid) ? globalThis.String(object.tenantGuid) : "",
      canPublish: isSet(object.canPublish) ? globalThis.Boolean(object.canPublish) : false,
      canSubscribe: isSet(object.canSubscribe) ? globalThis.Boolean(object.canSubscribe) : false,
      schemaId: isSet(object.schemaId) ? globalThis.String(object.schemaId) : "",
      rpcId: isSet(object.rpcId) ? globalThis.String(object.rpcId) : "",
    };
  },

  toJSON(message: TopicInfo): unknown {
    const obj: any = {};
    if (message.topicName !== "") {
      obj.topicName = message.topicName;
    }
    if (message.tenantGuid !== "") {
      obj.tenantGuid = message.tenantGuid;
    }
    if (message.canPublish !== false) {
      obj.canPublish = message.canPublish;
    }
    if (message.canSubscribe !== false) {
      obj.canSubscribe = message.canSubscribe;
    }
    if (message.schemaId !== "") {
      obj.schemaId = message.schemaId;
    }
    if (message.rpcId !== "") {
      obj.rpcId = message.rpcId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TopicInfo>, I>>(base?: I): TopicInfo {
    return TopicInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TopicInfo>, I>>(object: I): TopicInfo {
    const message = createBaseTopicInfo();
    message.topicName = object.topicName ?? "";
    message.tenantGuid = object.tenantGuid ?? "";
    message.canPublish = object.canPublish ?? false;
    message.canSubscribe = object.canSubscribe ?? false;
    message.schemaId = object.schemaId ?? "";
    message.rpcId = object.rpcId ?? "";
    return message;
  },
};

function createBaseTopicRequest(): TopicRequest {
  return { topicName: "" };
}

export const TopicRequest: MessageFns<TopicRequest> = {
  encode(message: TopicRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.topicName !== "") {
      writer.uint32(10).string(message.topicName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TopicRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTopicRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.topicName = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TopicRequest {
    return { topicName: isSet(object.topicName) ? globalThis.String(object.topicName) : "" };
  },

  toJSON(message: TopicRequest): unknown {
    const obj: any = {};
    if (message.topicName !== "") {
      obj.topicName = message.topicName;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TopicRequest>, I>>(base?: I): TopicRequest {
    return TopicRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TopicRequest>, I>>(object: I): TopicRequest {
    const message = createBaseTopicRequest();
    message.topicName = object.topicName ?? "";
    return message;
  },
};

function createBaseEventHeader(): EventHeader {
  return { key: "", value: Buffer.alloc(0) };
}

export const EventHeader: MessageFns<EventHeader> = {
  encode(message: EventHeader, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value.length !== 0) {
      writer.uint32(18).bytes(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): EventHeader {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEventHeader();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = Buffer.from(reader.bytes());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): EventHeader {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? Buffer.from(bytesFromBase64(object.value)) : Buffer.alloc(0),
    };
  },

  toJSON(message: EventHeader): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value.length !== 0) {
      obj.value = base64FromBytes(message.value);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<EventHeader>, I>>(base?: I): EventHeader {
    return EventHeader.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<EventHeader>, I>>(object: I): EventHeader {
    const message = createBaseEventHeader();
    message.key = object.key ?? "";
    message.value = object.value ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseProducerEvent(): ProducerEvent {
  return { id: "", schemaId: "", payload: Buffer.alloc(0), headers: [] };
}

export const ProducerEvent: MessageFns<ProducerEvent> = {
  encode(message: ProducerEvent, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    if (message.schemaId !== "") {
      writer.uint32(18).string(message.schemaId);
    }
    if (message.payload.length !== 0) {
      writer.uint32(26).bytes(message.payload);
    }
    for (const v of message.headers) {
      EventHeader.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ProducerEvent {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseProducerEvent();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.schemaId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.payload = Buffer.from(reader.bytes());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.headers.push(EventHeader.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ProducerEvent {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      schemaId: isSet(object.schemaId) ? globalThis.String(object.schemaId) : "",
      payload: isSet(object.payload) ? Buffer.from(bytesFromBase64(object.payload)) : Buffer.alloc(0),
      headers: globalThis.Array.isArray(object?.headers) ? object.headers.map((e: any) => EventHeader.fromJSON(e)) : [],
    };
  },

  toJSON(message: ProducerEvent): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.schemaId !== "") {
      obj.schemaId = message.schemaId;
    }
    if (message.payload.length !== 0) {
      obj.payload = base64FromBytes(message.payload);
    }
    if (message.headers?.length) {
      obj.headers = message.headers.map((e) => EventHeader.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ProducerEvent>, I>>(base?: I): ProducerEvent {
    return ProducerEvent.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ProducerEvent>, I>>(object: I): ProducerEvent {
    const message = createBaseProducerEvent();
    message.id = object.id ?? "";
    message.schemaId = object.schemaId ?? "";
    message.payload = object.payload ?? Buffer.alloc(0);
    message.headers = object.headers?.map((e) => EventHeader.fromPartial(e)) || [];
    return message;
  },
};

function createBaseConsumerEvent(): ConsumerEvent {
  return { event: undefined, replayId: Buffer.alloc(0) };
}

export const ConsumerEvent: MessageFns<ConsumerEvent> = {
  encode(message: ConsumerEvent, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.event !== undefined) {
      ProducerEvent.encode(message.event, writer.uint32(10).fork()).join();
    }
    if (message.replayId.length !== 0) {
      writer.uint32(18).bytes(message.replayId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ConsumerEvent {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseConsumerEvent();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.event = ProducerEvent.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.replayId = Buffer.from(reader.bytes());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ConsumerEvent {
    return {
      event: isSet(object.event) ? ProducerEvent.fromJSON(object.event) : undefined,
      replayId: isSet(object.replayId) ? Buffer.from(bytesFromBase64(object.replayId)) : Buffer.alloc(0),
    };
  },

  toJSON(message: ConsumerEvent): unknown {
    const obj: any = {};
    if (message.event !== undefined) {
      obj.event = ProducerEvent.toJSON(message.event);
    }
    if (message.replayId.length !== 0) {
      obj.replayId = base64FromBytes(message.replayId);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ConsumerEvent>, I>>(base?: I): ConsumerEvent {
    return ConsumerEvent.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ConsumerEvent>, I>>(object: I): ConsumerEvent {
    const message = createBaseConsumerEvent();
    message.event = (object.event !== undefined && object.event !== null)
      ? ProducerEvent.fromPartial(object.event)
      : undefined;
    message.replayId = object.replayId ?? Buffer.alloc(0);
    return message;
  },
};

function createBasePublishResult(): PublishResult {
  return { replayId: Buffer.alloc(0), error: undefined, correlationKey: "" };
}

export const PublishResult: MessageFns<PublishResult> = {
  encode(message: PublishResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.replayId.length !== 0) {
      writer.uint32(10).bytes(message.replayId);
    }
    if (message.error !== undefined) {
      Error.encode(message.error, writer.uint32(18).fork()).join();
    }
    if (message.correlationKey !== "") {
      writer.uint32(26).string(message.correlationKey);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PublishResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePublishResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.replayId = Buffer.from(reader.bytes());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.error = Error.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.correlationKey = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PublishResult {
    return {
      replayId: isSet(object.replayId) ? Buffer.from(bytesFromBase64(object.replayId)) : Buffer.alloc(0),
      error: isSet(object.error) ? Error.fromJSON(object.error) : undefined,
      correlationKey: isSet(object.correlationKey) ? globalThis.String(object.correlationKey) : "",
    };
  },

  toJSON(message: PublishResult): unknown {
    const obj: any = {};
    if (message.replayId.length !== 0) {
      obj.replayId = base64FromBytes(message.replayId);
    }
    if (message.error !== undefined) {
      obj.error = Error.toJSON(message.error);
    }
    if (message.correlationKey !== "") {
      obj.correlationKey = message.correlationKey;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PublishResult>, I>>(base?: I): PublishResult {
    return PublishResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PublishResult>, I>>(object: I): PublishResult {
    const message = createBasePublishResult();
    message.replayId = object.replayId ?? Buffer.alloc(0);
    message.error = (object.error !== undefined && object.error !== null) ? Error.fromPartial(object.error) : undefined;
    message.correlationKey = object.correlationKey ?? "";
    return message;
  },
};

function createBaseError(): Error {
  return { code: 0, msg: "" };
}

export const Error: MessageFns<Error> = {
  encode(message: Error, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.code !== 0) {
      writer.uint32(8).int32(message.code);
    }
    if (message.msg !== "") {
      writer.uint32(18).string(message.msg);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Error {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseError();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.code = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.msg = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Error {
    return {
      code: isSet(object.code) ? errorCodeFromJSON(object.code) : 0,
      msg: isSet(object.msg) ? globalThis.String(object.msg) : "",
    };
  },

  toJSON(message: Error): unknown {
    const obj: any = {};
    if (message.code !== 0) {
      obj.code = errorCodeToJSON(message.code);
    }
    if (message.msg !== "") {
      obj.msg = message.msg;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Error>, I>>(base?: I): Error {
    return Error.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Error>, I>>(object: I): Error {
    const message = createBaseError();
    message.code = object.code ?? 0;
    message.msg = object.msg ?? "";
    return message;
  },
};

function createBaseFetchRequest(): FetchRequest {
  return { topicName: "", replayPreset: 0, replayId: Buffer.alloc(0), numRequested: 0, authRefresh: "" };
}

export const FetchRequest: MessageFns<FetchRequest> = {
  encode(message: FetchRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.topicName !== "") {
      writer.uint32(10).string(message.topicName);
    }
    if (message.replayPreset !== 0) {
      writer.uint32(16).int32(message.replayPreset);
    }
    if (message.replayId.length !== 0) {
      writer.uint32(26).bytes(message.replayId);
    }
    if (message.numRequested !== 0) {
      writer.uint32(32).int32(message.numRequested);
    }
    if (message.authRefresh !== "") {
      writer.uint32(42).string(message.authRefresh);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FetchRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFetchRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.topicName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.replayPreset = reader.int32() as any;
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.replayId = Buffer.from(reader.bytes());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.numRequested = reader.int32();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.authRefresh = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FetchRequest {
    return {
      topicName: isSet(object.topicName) ? globalThis.String(object.topicName) : "",
      replayPreset: isSet(object.replayPreset) ? replayPresetFromJSON(object.replayPreset) : 0,
      replayId: isSet(object.replayId) ? Buffer.from(bytesFromBase64(object.replayId)) : Buffer.alloc(0),
      numRequested: isSet(object.numRequested) ? globalThis.Number(object.numRequested) : 0,
      authRefresh: isSet(object.authRefresh) ? globalThis.String(object.authRefresh) : "",
    };
  },

  toJSON(message: FetchRequest): unknown {
    const obj: any = {};
    if (message.topicName !== "") {
      obj.topicName = message.topicName;
    }
    if (message.replayPreset !== 0) {
      obj.replayPreset = replayPresetToJSON(message.replayPreset);
    }
    if (message.replayId.length !== 0) {
      obj.replayId = base64FromBytes(message.replayId);
    }
    if (message.numRequested !== 0) {
      obj.numRequested = Math.round(message.numRequested);
    }
    if (message.authRefresh !== "") {
      obj.authRefresh = message.authRefresh;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<FetchRequest>, I>>(base?: I): FetchRequest {
    return FetchRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<FetchRequest>, I>>(object: I): FetchRequest {
    const message = createBaseFetchRequest();
    message.topicName = object.topicName ?? "";
    message.replayPreset = object.replayPreset ?? 0;
    message.replayId = object.replayId ?? Buffer.alloc(0);
    message.numRequested = object.numRequested ?? 0;
    message.authRefresh = object.authRefresh ?? "";
    return message;
  },
};

function createBaseFetchResponse(): FetchResponse {
  return { events: [], latestReplayId: Buffer.alloc(0), rpcId: "", pendingNumRequested: 0 };
}

export const FetchResponse: MessageFns<FetchResponse> = {
  encode(message: FetchResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.events) {
      ConsumerEvent.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.latestReplayId.length !== 0) {
      writer.uint32(18).bytes(message.latestReplayId);
    }
    if (message.rpcId !== "") {
      writer.uint32(26).string(message.rpcId);
    }
    if (message.pendingNumRequested !== 0) {
      writer.uint32(32).int32(message.pendingNumRequested);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): FetchResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseFetchResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.events.push(ConsumerEvent.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.latestReplayId = Buffer.from(reader.bytes());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.rpcId = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.pendingNumRequested = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): FetchResponse {
    return {
      events: globalThis.Array.isArray(object?.events) ? object.events.map((e: any) => ConsumerEvent.fromJSON(e)) : [],
      latestReplayId: isSet(object.latestReplayId)
        ? Buffer.from(bytesFromBase64(object.latestReplayId))
        : Buffer.alloc(0),
      rpcId: isSet(object.rpcId) ? globalThis.String(object.rpcId) : "",
      pendingNumRequested: isSet(object.pendingNumRequested) ? globalThis.Number(object.pendingNumRequested) : 0,
    };
  },

  toJSON(message: FetchResponse): unknown {
    const obj: any = {};
    if (message.events?.length) {
      obj.events = message.events.map((e) => ConsumerEvent.toJSON(e));
    }
    if (message.latestReplayId.length !== 0) {
      obj.latestReplayId = base64FromBytes(message.latestReplayId);
    }
    if (message.rpcId !== "") {
      obj.rpcId = message.rpcId;
    }
    if (message.pendingNumRequested !== 0) {
      obj.pendingNumRequested = Math.round(message.pendingNumRequested);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<FetchResponse>, I>>(base?: I): FetchResponse {
    return FetchResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<FetchResponse>, I>>(object: I): FetchResponse {
    const message = createBaseFetchResponse();
    message.events = object.events?.map((e) => ConsumerEvent.fromPartial(e)) || [];
    message.latestReplayId = object.latestReplayId ?? Buffer.alloc(0);
    message.rpcId = object.rpcId ?? "";
    message.pendingNumRequested = object.pendingNumRequested ?? 0;
    return message;
  },
};

function createBaseSchemaRequest(): SchemaRequest {
  return { schemaId: "" };
}

export const SchemaRequest: MessageFns<SchemaRequest> = {
  encode(message: SchemaRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.schemaId !== "") {
      writer.uint32(10).string(message.schemaId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SchemaRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSchemaRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.schemaId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SchemaRequest {
    return { schemaId: isSet(object.schemaId) ? globalThis.String(object.schemaId) : "" };
  },

  toJSON(message: SchemaRequest): unknown {
    const obj: any = {};
    if (message.schemaId !== "") {
      obj.schemaId = message.schemaId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SchemaRequest>, I>>(base?: I): SchemaRequest {
    return SchemaRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SchemaRequest>, I>>(object: I): SchemaRequest {
    const message = createBaseSchemaRequest();
    message.schemaId = object.schemaId ?? "";
    return message;
  },
};

function createBaseSchemaInfo(): SchemaInfo {
  return { schemaJson: "", schemaId: "", rpcId: "" };
}

export const SchemaInfo: MessageFns<SchemaInfo> = {
  encode(message: SchemaInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.schemaJson !== "") {
      writer.uint32(10).string(message.schemaJson);
    }
    if (message.schemaId !== "") {
      writer.uint32(18).string(message.schemaId);
    }
    if (message.rpcId !== "") {
      writer.uint32(26).string(message.rpcId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SchemaInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSchemaInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.schemaJson = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.schemaId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.rpcId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SchemaInfo {
    return {
      schemaJson: isSet(object.schemaJson) ? globalThis.String(object.schemaJson) : "",
      schemaId: isSet(object.schemaId) ? globalThis.String(object.schemaId) : "",
      rpcId: isSet(object.rpcId) ? globalThis.String(object.rpcId) : "",
    };
  },

  toJSON(message: SchemaInfo): unknown {
    const obj: any = {};
    if (message.schemaJson !== "") {
      obj.schemaJson = message.schemaJson;
    }
    if (message.schemaId !== "") {
      obj.schemaId = message.schemaId;
    }
    if (message.rpcId !== "") {
      obj.rpcId = message.rpcId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SchemaInfo>, I>>(base?: I): SchemaInfo {
    return SchemaInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SchemaInfo>, I>>(object: I): SchemaInfo {
    const message = createBaseSchemaInfo();
    message.schemaJson = object.schemaJson ?? "";
    message.schemaId = object.schemaId ?? "";
    message.rpcId = object.rpcId ?? "";
    return message;
  },
};

function createBasePublishRequest(): PublishRequest {
  return { topicName: "", events: [], authRefresh: "" };
}

export const PublishRequest: MessageFns<PublishRequest> = {
  encode(message: PublishRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.topicName !== "") {
      writer.uint32(10).string(message.topicName);
    }
    for (const v of message.events) {
      ProducerEvent.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.authRefresh !== "") {
      writer.uint32(26).string(message.authRefresh);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PublishRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePublishRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.topicName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.events.push(ProducerEvent.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.authRefresh = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PublishRequest {
    return {
      topicName: isSet(object.topicName) ? globalThis.String(object.topicName) : "",
      events: globalThis.Array.isArray(object?.events) ? object.events.map((e: any) => ProducerEvent.fromJSON(e)) : [],
      authRefresh: isSet(object.authRefresh) ? globalThis.String(object.authRefresh) : "",
    };
  },

  toJSON(message: PublishRequest): unknown {
    const obj: any = {};
    if (message.topicName !== "") {
      obj.topicName = message.topicName;
    }
    if (message.events?.length) {
      obj.events = message.events.map((e) => ProducerEvent.toJSON(e));
    }
    if (message.authRefresh !== "") {
      obj.authRefresh = message.authRefresh;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PublishRequest>, I>>(base?: I): PublishRequest {
    return PublishRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PublishRequest>, I>>(object: I): PublishRequest {
    const message = createBasePublishRequest();
    message.topicName = object.topicName ?? "";
    message.events = object.events?.map((e) => ProducerEvent.fromPartial(e)) || [];
    message.authRefresh = object.authRefresh ?? "";
    return message;
  },
};

function createBasePublishResponse(): PublishResponse {
  return { results: [], schemaId: "", rpcId: "" };
}

export const PublishResponse: MessageFns<PublishResponse> = {
  encode(message: PublishResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      PublishResult.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.schemaId !== "") {
      writer.uint32(18).string(message.schemaId);
    }
    if (message.rpcId !== "") {
      writer.uint32(26).string(message.rpcId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): PublishResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePublishResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(PublishResult.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.schemaId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.rpcId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PublishResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => PublishResult.fromJSON(e))
        : [],
      schemaId: isSet(object.schemaId) ? globalThis.String(object.schemaId) : "",
      rpcId: isSet(object.rpcId) ? globalThis.String(object.rpcId) : "",
    };
  },

  toJSON(message: PublishResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => PublishResult.toJSON(e));
    }
    if (message.schemaId !== "") {
      obj.schemaId = message.schemaId;
    }
    if (message.rpcId !== "") {
      obj.rpcId = message.rpcId;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<PublishResponse>, I>>(base?: I): PublishResponse {
    return PublishResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<PublishResponse>, I>>(object: I): PublishResponse {
    const message = createBasePublishResponse();
    message.results = object.results?.map((e) => PublishResult.fromPartial(e)) || [];
    message.schemaId = object.schemaId ?? "";
    message.rpcId = object.rpcId ?? "";
    return message;
  },
};

function createBaseManagedFetchRequest(): ManagedFetchRequest {
  return { subscriptionId: "", developerName: "", numRequested: 0, authRefresh: "", commitReplayIdRequest: undefined };
}

export const ManagedFetchRequest: MessageFns<ManagedFetchRequest> = {
  encode(message: ManagedFetchRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.subscriptionId !== "") {
      writer.uint32(10).string(message.subscriptionId);
    }
    if (message.developerName !== "") {
      writer.uint32(18).string(message.developerName);
    }
    if (message.numRequested !== 0) {
      writer.uint32(24).int32(message.numRequested);
    }
    if (message.authRefresh !== "") {
      writer.uint32(34).string(message.authRefresh);
    }
    if (message.commitReplayIdRequest !== undefined) {
      CommitReplayRequest.encode(message.commitReplayIdRequest, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ManagedFetchRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseManagedFetchRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.subscriptionId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.developerName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.numRequested = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.authRefresh = reader.string();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.commitReplayIdRequest = CommitReplayRequest.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ManagedFetchRequest {
    return {
      subscriptionId: isSet(object.subscriptionId) ? globalThis.String(object.subscriptionId) : "",
      developerName: isSet(object.developerName) ? globalThis.String(object.developerName) : "",
      numRequested: isSet(object.numRequested) ? globalThis.Number(object.numRequested) : 0,
      authRefresh: isSet(object.authRefresh) ? globalThis.String(object.authRefresh) : "",
      commitReplayIdRequest: isSet(object.commitReplayIdRequest)
        ? CommitReplayRequest.fromJSON(object.commitReplayIdRequest)
        : undefined,
    };
  },

  toJSON(message: ManagedFetchRequest): unknown {
    const obj: any = {};
    if (message.subscriptionId !== "") {
      obj.subscriptionId = message.subscriptionId;
    }
    if (message.developerName !== "") {
      obj.developerName = message.developerName;
    }
    if (message.numRequested !== 0) {
      obj.numRequested = Math.round(message.numRequested);
    }
    if (message.authRefresh !== "") {
      obj.authRefresh = message.authRefresh;
    }
    if (message.commitReplayIdRequest !== undefined) {
      obj.commitReplayIdRequest = CommitReplayRequest.toJSON(message.commitReplayIdRequest);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ManagedFetchRequest>, I>>(base?: I): ManagedFetchRequest {
    return ManagedFetchRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ManagedFetchRequest>, I>>(object: I): ManagedFetchRequest {
    const message = createBaseManagedFetchRequest();
    message.subscriptionId = object.subscriptionId ?? "";
    message.developerName = object.developerName ?? "";
    message.numRequested = object.numRequested ?? 0;
    message.authRefresh = object.authRefresh ?? "";
    message.commitReplayIdRequest =
      (object.commitReplayIdRequest !== undefined && object.commitReplayIdRequest !== null)
        ? CommitReplayRequest.fromPartial(object.commitReplayIdRequest)
        : undefined;
    return message;
  },
};

function createBaseManagedFetchResponse(): ManagedFetchResponse {
  return { events: [], latestReplayId: Buffer.alloc(0), rpcId: "", pendingNumRequested: 0, commitResponse: undefined };
}

export const ManagedFetchResponse: MessageFns<ManagedFetchResponse> = {
  encode(message: ManagedFetchResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.events) {
      ConsumerEvent.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.latestReplayId.length !== 0) {
      writer.uint32(18).bytes(message.latestReplayId);
    }
    if (message.rpcId !== "") {
      writer.uint32(26).string(message.rpcId);
    }
    if (message.pendingNumRequested !== 0) {
      writer.uint32(32).int32(message.pendingNumRequested);
    }
    if (message.commitResponse !== undefined) {
      CommitReplayResponse.encode(message.commitResponse, writer.uint32(42).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ManagedFetchResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseManagedFetchResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.events.push(ConsumerEvent.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.latestReplayId = Buffer.from(reader.bytes());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.rpcId = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.pendingNumRequested = reader.int32();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.commitResponse = CommitReplayResponse.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ManagedFetchResponse {
    return {
      events: globalThis.Array.isArray(object?.events) ? object.events.map((e: any) => ConsumerEvent.fromJSON(e)) : [],
      latestReplayId: isSet(object.latestReplayId)
        ? Buffer.from(bytesFromBase64(object.latestReplayId))
        : Buffer.alloc(0),
      rpcId: isSet(object.rpcId) ? globalThis.String(object.rpcId) : "",
      pendingNumRequested: isSet(object.pendingNumRequested) ? globalThis.Number(object.pendingNumRequested) : 0,
      commitResponse: isSet(object.commitResponse) ? CommitReplayResponse.fromJSON(object.commitResponse) : undefined,
    };
  },

  toJSON(message: ManagedFetchResponse): unknown {
    const obj: any = {};
    if (message.events?.length) {
      obj.events = message.events.map((e) => ConsumerEvent.toJSON(e));
    }
    if (message.latestReplayId.length !== 0) {
      obj.latestReplayId = base64FromBytes(message.latestReplayId);
    }
    if (message.rpcId !== "") {
      obj.rpcId = message.rpcId;
    }
    if (message.pendingNumRequested !== 0) {
      obj.pendingNumRequested = Math.round(message.pendingNumRequested);
    }
    if (message.commitResponse !== undefined) {
      obj.commitResponse = CommitReplayResponse.toJSON(message.commitResponse);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ManagedFetchResponse>, I>>(base?: I): ManagedFetchResponse {
    return ManagedFetchResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ManagedFetchResponse>, I>>(object: I): ManagedFetchResponse {
    const message = createBaseManagedFetchResponse();
    message.events = object.events?.map((e) => ConsumerEvent.fromPartial(e)) || [];
    message.latestReplayId = object.latestReplayId ?? Buffer.alloc(0);
    message.rpcId = object.rpcId ?? "";
    message.pendingNumRequested = object.pendingNumRequested ?? 0;
    message.commitResponse = (object.commitResponse !== undefined && object.commitResponse !== null)
      ? CommitReplayResponse.fromPartial(object.commitResponse)
      : undefined;
    return message;
  },
};

function createBaseCommitReplayRequest(): CommitReplayRequest {
  return { commitRequestId: "", replayId: Buffer.alloc(0) };
}

export const CommitReplayRequest: MessageFns<CommitReplayRequest> = {
  encode(message: CommitReplayRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commitRequestId !== "") {
      writer.uint32(10).string(message.commitRequestId);
    }
    if (message.replayId.length !== 0) {
      writer.uint32(18).bytes(message.replayId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CommitReplayRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCommitReplayRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.commitRequestId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.replayId = Buffer.from(reader.bytes());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CommitReplayRequest {
    return {
      commitRequestId: isSet(object.commitRequestId) ? globalThis.String(object.commitRequestId) : "",
      replayId: isSet(object.replayId) ? Buffer.from(bytesFromBase64(object.replayId)) : Buffer.alloc(0),
    };
  },

  toJSON(message: CommitReplayRequest): unknown {
    const obj: any = {};
    if (message.commitRequestId !== "") {
      obj.commitRequestId = message.commitRequestId;
    }
    if (message.replayId.length !== 0) {
      obj.replayId = base64FromBytes(message.replayId);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<CommitReplayRequest>, I>>(base?: I): CommitReplayRequest {
    return CommitReplayRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<CommitReplayRequest>, I>>(object: I): CommitReplayRequest {
    const message = createBaseCommitReplayRequest();
    message.commitRequestId = object.commitRequestId ?? "";
    message.replayId = object.replayId ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseCommitReplayResponse(): CommitReplayResponse {
  return { commitRequestId: "", replayId: Buffer.alloc(0), error: undefined, processTime: 0 };
}

export const CommitReplayResponse: MessageFns<CommitReplayResponse> = {
  encode(message: CommitReplayResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.commitRequestId !== "") {
      writer.uint32(10).string(message.commitRequestId);
    }
    if (message.replayId.length !== 0) {
      writer.uint32(18).bytes(message.replayId);
    }
    if (message.error !== undefined) {
      Error.encode(message.error, writer.uint32(26).fork()).join();
    }
    if (message.processTime !== 0) {
      writer.uint32(32).int64(message.processTime);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CommitReplayResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCommitReplayResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.commitRequestId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.replayId = Buffer.from(reader.bytes());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.error = Error.decode(reader, reader.uint32());
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.processTime = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CommitReplayResponse {
    return {
      commitRequestId: isSet(object.commitRequestId) ? globalThis.String(object.commitRequestId) : "",
      replayId: isSet(object.replayId) ? Buffer.from(bytesFromBase64(object.replayId)) : Buffer.alloc(0),
      error: isSet(object.error) ? Error.fromJSON(object.error) : undefined,
      processTime: isSet(object.processTime) ? globalThis.Number(object.processTime) : 0,
    };
  },

  toJSON(message: CommitReplayResponse): unknown {
    const obj: any = {};
    if (message.commitRequestId !== "") {
      obj.commitRequestId = message.commitRequestId;
    }
    if (message.replayId.length !== 0) {
      obj.replayId = base64FromBytes(message.replayId);
    }
    if (message.error !== undefined) {
      obj.error = Error.toJSON(message.error);
    }
    if (message.processTime !== 0) {
      obj.processTime = Math.round(message.processTime);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<CommitReplayResponse>, I>>(base?: I): CommitReplayResponse {
    return CommitReplayResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<CommitReplayResponse>, I>>(object: I): CommitReplayResponse {
    const message = createBaseCommitReplayResponse();
    message.commitRequestId = object.commitRequestId ?? "";
    message.replayId = object.replayId ?? Buffer.alloc(0);
    message.error = (object.error !== undefined && object.error !== null) ? Error.fromPartial(object.error) : undefined;
    message.processTime = object.processTime ?? 0;
    return message;
  },
};

/**
 * The Pub/Sub API provides a single interface for publishing and subscribing to platform events, including real-time
 * event monitoring events, and change data capture events. The Pub/Sub API is a gRPC API that is based on HTTP/2.
 *
 * A session token is needed to authenticate. Any of the Salesforce supported
 * OAuth flows can be used to obtain a session token:
 * https://help.salesforce.com/articleView?id=sf.remoteaccess_oauth_flows.htm&type=5
 *
 * For each RPC, a client needs to pass authentication information
 * as metadata headers (https://www.grpc.io/docs/guides/concepts/#metadata) with their method call.
 *
 * For Salesforce session token authentication, use:
 *   accesstoken : access token
 *   instanceurl : Salesforce instance URL
 *   tenantid : tenant/org id of the client
 *
 * StatusException is thrown in case of response failure for any request.
 */
export type PubSubService = typeof PubSubService;
export const PubSubService = {
  /**
   * Bidirectional streaming RPC to subscribe to a Topic. The subscription is pull-based. A client can request
   * for more events as it consumes events. This enables a client to handle flow control based on the client's processing speed.
   *
   * Typical flow:
   * 1. Client requests for X number of events via FetchRequest.
   * 2. Server receives request and delivers events until X events are delivered to the client via one or more FetchResponse messages.
   * 3. Client consumes the FetchResponse messages as they come.
   * 4. Client issues new FetchRequest for Y more number of events. This request can
   *    come before the server has delivered the earlier requested X number of events
   *    so the client gets a continuous stream of events if any.
   *
   * If a client requests more events before the server finishes the last
   * requested amount, the server appends the new amount to the current amount of
   * events it still needs to fetch and deliver.
   *
   * A client can subscribe at any point in the stream by providing a replay option in the first FetchRequest.
   * The replay option is honored for the first FetchRequest received from a client. Any subsequent FetchRequests with a
   * new replay option are ignored. A client needs to call the Subscribe RPC again to restart the subscription
   * at a new point in the stream.
   *
   * The first FetchRequest of the stream identifies the topic to subscribe to.
   * If any subsequent FetchRequest provides topic_name, it must match what
   * was provided in the first FetchRequest; otherwise, the RPC returns an error
   * with INVALID_ARGUMENT status.
   */
  subscribe: {
    path: "/eventbus.v1.PubSub/Subscribe",
    requestStream: true,
    responseStream: true,
    requestSerialize: (value: FetchRequest) => Buffer.from(FetchRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => FetchRequest.decode(value),
    responseSerialize: (value: FetchResponse) => Buffer.from(FetchResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => FetchResponse.decode(value),
  },
  /** Get the event schema for a topic based on a schema ID. */
  getSchema: {
    path: "/eventbus.v1.PubSub/GetSchema",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: SchemaRequest) => Buffer.from(SchemaRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => SchemaRequest.decode(value),
    responseSerialize: (value: SchemaInfo) => Buffer.from(SchemaInfo.encode(value).finish()),
    responseDeserialize: (value: Buffer) => SchemaInfo.decode(value),
  },
  /** Get the topic Information related to the specified topic. */
  getTopic: {
    path: "/eventbus.v1.PubSub/GetTopic",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: TopicRequest) => Buffer.from(TopicRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => TopicRequest.decode(value),
    responseSerialize: (value: TopicInfo) => Buffer.from(TopicInfo.encode(value).finish()),
    responseDeserialize: (value: Buffer) => TopicInfo.decode(value),
  },
  /** Send a publish request to synchronously publish events to a topic. */
  publish: {
    path: "/eventbus.v1.PubSub/Publish",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: PublishRequest) => Buffer.from(PublishRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => PublishRequest.decode(value),
    responseSerialize: (value: PublishResponse) => Buffer.from(PublishResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => PublishResponse.decode(value),
  },
  /**
   * Bidirectional Streaming RPC to publish events to the event bus.
   * PublishRequest contains the batch of events to publish.
   *
   * The first PublishRequest of the stream identifies the topic to publish on.
   * If any subsequent PublishRequest provides topic_name, it must match what
   * was provided in the first PublishRequest; otherwise, the RPC returns an error
   * with INVALID_ARGUMENT status.
   *
   * The server returns a PublishResponse for each PublishRequest when publish is
   * complete for the batch. A client does not have to wait for a PublishResponse
   * before sending a new PublishRequest, i.e. multiple publish batches can be queued
   * up, which allows for higher publish rate as a client can asynchronously
   * publish more events while publishes are still in flight on the server side.
   *
   * PublishResponse holds a PublishResult for each event published that indicates success
   * or failure of the publish. A client can then retry the publish as needed before sending
   * more PublishRequests for new events to publish.
   *
   * A client must send a valid publish request with one or more events every 70 seconds to hold on to the stream.
   * Otherwise, the server closes the stream and notifies the client. Once the client is notified of the stream closure,
   * it must make a new PublishStream call to resume publishing.
   */
  publishStream: {
    path: "/eventbus.v1.PubSub/PublishStream",
    requestStream: true,
    responseStream: true,
    requestSerialize: (value: PublishRequest) => Buffer.from(PublishRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => PublishRequest.decode(value),
    responseSerialize: (value: PublishResponse) => Buffer.from(PublishResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => PublishResponse.decode(value),
  },
  /**
   * This feature is part of an open beta release and is subject to the applicable
   * Beta Services Terms provided at Agreements and Terms
   * (https://www.salesforce.com/company/legal/agreements/).
   *
   * Same as Subscribe, but for Managed Subscription clients.
   * This feature is part of an open beta release.
   */
  managedSubscribe: {
    path: "/eventbus.v1.PubSub/ManagedSubscribe",
    requestStream: true,
    responseStream: true,
    requestSerialize: (value: ManagedFetchRequest) => Buffer.from(ManagedFetchRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => ManagedFetchRequest.decode(value),
    responseSerialize: (value: ManagedFetchResponse) => Buffer.from(ManagedFetchResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => ManagedFetchResponse.decode(value),
  },
} as const;

export interface PubSubServer extends UntypedServiceImplementation {
  /**
   * Bidirectional streaming RPC to subscribe to a Topic. The subscription is pull-based. A client can request
   * for more events as it consumes events. This enables a client to handle flow control based on the client's processing speed.
   *
   * Typical flow:
   * 1. Client requests for X number of events via FetchRequest.
   * 2. Server receives request and delivers events until X events are delivered to the client via one or more FetchResponse messages.
   * 3. Client consumes the FetchResponse messages as they come.
   * 4. Client issues new FetchRequest for Y more number of events. This request can
   *    come before the server has delivered the earlier requested X number of events
   *    so the client gets a continuous stream of events if any.
   *
   * If a client requests more events before the server finishes the last
   * requested amount, the server appends the new amount to the current amount of
   * events it still needs to fetch and deliver.
   *
   * A client can subscribe at any point in the stream by providing a replay option in the first FetchRequest.
   * The replay option is honored for the first FetchRequest received from a client. Any subsequent FetchRequests with a
   * new replay option are ignored. A client needs to call the Subscribe RPC again to restart the subscription
   * at a new point in the stream.
   *
   * The first FetchRequest of the stream identifies the topic to subscribe to.
   * If any subsequent FetchRequest provides topic_name, it must match what
   * was provided in the first FetchRequest; otherwise, the RPC returns an error
   * with INVALID_ARGUMENT status.
   */
  subscribe: handleBidiStreamingCall<FetchRequest, FetchResponse>;
  /** Get the event schema for a topic based on a schema ID. */
  getSchema: handleUnaryCall<SchemaRequest, SchemaInfo>;
  /** Get the topic Information related to the specified topic. */
  getTopic: handleUnaryCall<TopicRequest, TopicInfo>;
  /** Send a publish request to synchronously publish events to a topic. */
  publish: handleUnaryCall<PublishRequest, PublishResponse>;
  /**
   * Bidirectional Streaming RPC to publish events to the event bus.
   * PublishRequest contains the batch of events to publish.
   *
   * The first PublishRequest of the stream identifies the topic to publish on.
   * If any subsequent PublishRequest provides topic_name, it must match what
   * was provided in the first PublishRequest; otherwise, the RPC returns an error
   * with INVALID_ARGUMENT status.
   *
   * The server returns a PublishResponse for each PublishRequest when publish is
   * complete for the batch. A client does not have to wait for a PublishResponse
   * before sending a new PublishRequest, i.e. multiple publish batches can be queued
   * up, which allows for higher publish rate as a client can asynchronously
   * publish more events while publishes are still in flight on the server side.
   *
   * PublishResponse holds a PublishResult for each event published that indicates success
   * or failure of the publish. A client can then retry the publish as needed before sending
   * more PublishRequests for new events to publish.
   *
   * A client must send a valid publish request with one or more events every 70 seconds to hold on to the stream.
   * Otherwise, the server closes the stream and notifies the client. Once the client is notified of the stream closure,
   * it must make a new PublishStream call to resume publishing.
   */
  publishStream: handleBidiStreamingCall<PublishRequest, PublishResponse>;
  /**
   * This feature is part of an open beta release and is subject to the applicable
   * Beta Services Terms provided at Agreements and Terms
   * (https://www.salesforce.com/company/legal/agreements/).
   *
   * Same as Subscribe, but for Managed Subscription clients.
   * This feature is part of an open beta release.
   */
  managedSubscribe: handleBidiStreamingCall<ManagedFetchRequest, ManagedFetchResponse>;
}

export interface PubSubClient extends Client {
  /**
   * Bidirectional streaming RPC to subscribe to a Topic. The subscription is pull-based. A client can request
   * for more events as it consumes events. This enables a client to handle flow control based on the client's processing speed.
   *
   * Typical flow:
   * 1. Client requests for X number of events via FetchRequest.
   * 2. Server receives request and delivers events until X events are delivered to the client via one or more FetchResponse messages.
   * 3. Client consumes the FetchResponse messages as they come.
   * 4. Client issues new FetchRequest for Y more number of events. This request can
   *    come before the server has delivered the earlier requested X number of events
   *    so the client gets a continuous stream of events if any.
   *
   * If a client requests more events before the server finishes the last
   * requested amount, the server appends the new amount to the current amount of
   * events it still needs to fetch and deliver.
   *
   * A client can subscribe at any point in the stream by providing a replay option in the first FetchRequest.
   * The replay option is honored for the first FetchRequest received from a client. Any subsequent FetchRequests with a
   * new replay option are ignored. A client needs to call the Subscribe RPC again to restart the subscription
   * at a new point in the stream.
   *
   * The first FetchRequest of the stream identifies the topic to subscribe to.
   * If any subsequent FetchRequest provides topic_name, it must match what
   * was provided in the first FetchRequest; otherwise, the RPC returns an error
   * with INVALID_ARGUMENT status.
   */
  subscribe(): ClientDuplexStream<FetchRequest, FetchResponse>;
  subscribe(options: Partial<CallOptions>): ClientDuplexStream<FetchRequest, FetchResponse>;
  subscribe(metadata: Metadata, options?: Partial<CallOptions>): ClientDuplexStream<FetchRequest, FetchResponse>;
  /** Get the event schema for a topic based on a schema ID. */
  getSchema(
    request: SchemaRequest,
    callback: (error: ServiceError | null, response: SchemaInfo) => void,
  ): ClientUnaryCall;
  getSchema(
    request: SchemaRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: SchemaInfo) => void,
  ): ClientUnaryCall;
  getSchema(
    request: SchemaRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: SchemaInfo) => void,
  ): ClientUnaryCall;
  /** Get the topic Information related to the specified topic. */
  getTopic(request: TopicRequest, callback: (error: ServiceError | null, response: TopicInfo) => void): ClientUnaryCall;
  getTopic(
    request: TopicRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TopicInfo) => void,
  ): ClientUnaryCall;
  getTopic(
    request: TopicRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TopicInfo) => void,
  ): ClientUnaryCall;
  /** Send a publish request to synchronously publish events to a topic. */
  publish(
    request: PublishRequest,
    callback: (error: ServiceError | null, response: PublishResponse) => void,
  ): ClientUnaryCall;
  publish(
    request: PublishRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: PublishResponse) => void,
  ): ClientUnaryCall;
  publish(
    request: PublishRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: PublishResponse) => void,
  ): ClientUnaryCall;
  /**
   * Bidirectional Streaming RPC to publish events to the event bus.
   * PublishRequest contains the batch of events to publish.
   *
   * The first PublishRequest of the stream identifies the topic to publish on.
   * If any subsequent PublishRequest provides topic_name, it must match what
   * was provided in the first PublishRequest; otherwise, the RPC returns an error
   * with INVALID_ARGUMENT status.
   *
   * The server returns a PublishResponse for each PublishRequest when publish is
   * complete for the batch. A client does not have to wait for a PublishResponse
   * before sending a new PublishRequest, i.e. multiple publish batches can be queued
   * up, which allows for higher publish rate as a client can asynchronously
   * publish more events while publishes are still in flight on the server side.
   *
   * PublishResponse holds a PublishResult for each event published that indicates success
   * or failure of the publish. A client can then retry the publish as needed before sending
   * more PublishRequests for new events to publish.
   *
   * A client must send a valid publish request with one or more events every 70 seconds to hold on to the stream.
   * Otherwise, the server closes the stream and notifies the client. Once the client is notified of the stream closure,
   * it must make a new PublishStream call to resume publishing.
   */
  publishStream(): ClientDuplexStream<PublishRequest, PublishResponse>;
  publishStream(options: Partial<CallOptions>): ClientDuplexStream<PublishRequest, PublishResponse>;
  publishStream(
    metadata: Metadata,
    options?: Partial<CallOptions>,
  ): ClientDuplexStream<PublishRequest, PublishResponse>;
  /**
   * This feature is part of an open beta release and is subject to the applicable
   * Beta Services Terms provided at Agreements and Terms
   * (https://www.salesforce.com/company/legal/agreements/).
   *
   * Same as Subscribe, but for Managed Subscription clients.
   * This feature is part of an open beta release.
   */
  managedSubscribe(): ClientDuplexStream<ManagedFetchRequest, ManagedFetchResponse>;
  managedSubscribe(options: Partial<CallOptions>): ClientDuplexStream<ManagedFetchRequest, ManagedFetchResponse>;
  managedSubscribe(
    metadata: Metadata,
    options?: Partial<CallOptions>,
  ): ClientDuplexStream<ManagedFetchRequest, ManagedFetchResponse>;
}

export const PubSubClient = makeGenericClientConstructor(PubSubService, "eventbus.v1.PubSub") as unknown as {
  new (address: string, credentials: ChannelCredentials, options?: Partial<ClientOptions>): PubSubClient;
  service: typeof PubSubService;
  serviceName: string;
};

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function longToNumber(int64: { toString(): string }): number {
  const num = globalThis.Number(int64.toString());
  if (num > globalThis.Number.MAX_SAFE_INTEGER) {
    throw new globalThis.Error("Value is larger than Number.MAX_SAFE_INTEGER");
  }
  if (num < globalThis.Number.MIN_SAFE_INTEGER) {
    throw new globalThis.Error("Value is smaller than Number.MIN_SAFE_INTEGER");
  }
  return num;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
